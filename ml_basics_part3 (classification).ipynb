{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef77d0cd-b473-45cb-8e23-846426203aa0",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier with the Iris Dataset\n",
    "\n",
    "##  Overview\n",
    "This code demonstrates how to use a **Decision Tree Classifier** from `scikit-learn` to classify flowers in the **Iris dataset**.  \n",
    "The Iris dataset is a well-known dataset in machine learning that contains measurements of iris flowers (features) and their species (labels).  \n",
    "\n",
    "The goal: **Train a Decision Tree to predict the species of iris flowers** based on their measurements.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps in the Code\n",
    "\n",
    "### 1. Import Libraries\n",
    "- **pandas, numpy** ‚Üí for handling data (not heavily used here, but good practice).  \n",
    "- **scikit-learn (sklearn)** ‚Üí for dataset loading, splitting, training, and evaluation.  \n",
    "\n",
    "### 2. Load the Dataset\n",
    "- `datasets.load_iris()` loads the Iris dataset.  \n",
    "- `X = iris.data` ‚Üí the features (measurements of the flowers).  \n",
    "- `y = iris.target` ‚Üí the labels (species: Setosa, Versicolor, Virginica).  \n",
    "\n",
    "### 3. Split the Data\n",
    "- `train_test_split()` splits the dataset into:\n",
    "  - **80% training data** ‚Üí used to train the model.  \n",
    "  - **20% testing data** ‚Üí used to check accuracy.  \n",
    "\n",
    "### 4. Train the Model\n",
    "- `DecisionTreeClassifier()` creates a Decision Tree model.  \n",
    "- `.fit(X_train, y_train)` trains it on the training data.  \n",
    "\n",
    "### 5. Make Predictions\n",
    "- `.predict(X_test)` predicts flower species for the test data.  \n",
    "\n",
    "### 6. Evaluate the Model\n",
    "- **Accuracy** ‚Üí How many predictions were correct overall.  \n",
    "- **Precision** ‚Üí How many predicted classes were actually correct.  \n",
    "- **Recall** ‚Üí How well the model found all correct samples.  \n",
    "- **F1-score** ‚Üí A balance between precision and recall.  \n",
    "\n",
    "### 7. Print Results\n",
    "The performance metrics are displayed to show how well the model performed.  \n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "This example shows a complete workflow for **classification using Decision Trees**:\n",
    "- Load dataset  \n",
    "- Split data  \n",
    "- Train model  \n",
    "- Make predictions  \n",
    "- Evaluate performance  \n",
    "\n",
    "The Iris dataset is a simple but powerful starting point for learning classification in machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fdae906-28a2-4a15-a32b-07c48a26e316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries and Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,  f1_score, classification_report\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data # Features\n",
    "y = iris.target # Labels\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for test data\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Compute performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6085ab9-3ccb-420e-9653-1f5d2aac1953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda931c-6c28-445c-8c24-2c5c32f44cb7",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (kNN) Classifier\n",
    "\n",
    "##  Overview\n",
    "This code demonstrates the **k-Nearest Neighbors (kNN)** algorithm for **binary classification**.  \n",
    "Instead of using a real dataset, we generate a **synthetic dataset** with `make_classification()` from scikit-learn.  \n",
    "\n",
    "The goal: Train a kNN classifier to correctly classify samples into two classes (0 or 1).\n",
    "\n",
    "---\n",
    "\n",
    "##  Steps in the Code\n",
    "\n",
    "### 1. Import Libraries\n",
    "- `make_classification` ‚Üí generates a synthetic dataset.  \n",
    "- `train_test_split` ‚Üí splits data into train/test sets.  \n",
    "- `KNeighborsClassifier` ‚Üí the kNN algorithm.  \n",
    "- `accuracy_score`, `precision_score`, `recall_score`, `f1_score` ‚Üí evaluation metrics.  \n",
    "\n",
    "### 2. Generate Dataset\n",
    "- `make_classification(n_samples=1000, n_features=20, n_classes=2)`  \n",
    "  - Creates **1000 samples**.  \n",
    "  - Each sample has **20 features**.  \n",
    "  - There are **2 possible classes** (binary classification).  \n",
    "\n",
    "### 3. Split the Data\n",
    "- `train_test_split()` splits into:\n",
    "  - **80% training data** (to train the model).  \n",
    "  - **20% testing data** (to evaluate the model).  \n",
    "\n",
    "### 4. Train the kNN Model\n",
    "- `KNeighborsClassifier(n_neighbors=5)` ‚Üí uses the **5 nearest neighbors** to classify a sample.  \n",
    "- `.fit(X_train, y_train)` trains the model.  \n",
    "\n",
    "### 5. Make Predictions\n",
    "- `.predict(X_test)` predicts the class labels for test data.  \n",
    "\n",
    "### 6. Evaluate the Model\n",
    "The following performance metrics are calculated:\n",
    "- **Accuracy** ‚Üí Overall correctness of the model.  \n",
    "- **Precision** ‚Üí Of the predicted positives, how many were correct.  \n",
    "- **Recall** ‚Üí Of the actual positives, how many were correctly identified.  \n",
    "- **F1-score** ‚Üí Balance between precision and recall.  \n",
    "\n",
    "### 7. Print Results\n",
    "The performance metrics are displayed with 4 decimal places for readability.  \n",
    "\n",
    "---\n",
    "\n",
    "##  Summary\n",
    "This example shows how to:  \n",
    "1. Generate a synthetic dataset.  \n",
    "2. Train a **k-Nearest Neighbors (kNN)** model.  \n",
    "3. Evaluate it using **common classification metrics**.  \n",
    "\n",
    "kNN is a simple yet powerful algorithm that classifies a new data point based on the majority class of its **nearest neighbors**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c52f02f-6543-4db0-b04f-a5ddb7f9d948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics:\n",
      "Accuracy: 0.8100\n",
      "Precision: 0.8791\n",
      "Recall: 0.7477\n",
      "F1-score: 0.8081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Generate synthetic dataset for binary classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train kNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Output performance metrics\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921041f6-f482-4d8c-a634-d110bf6fb163",
   "metadata": {},
   "source": [
    "#  Support Vector Machine (SVM) with ROC Curve\n",
    "\n",
    "##  Overview\n",
    "This code shows how to train a **Support Vector Machine (SVM)** classifier for binary classification, and then evaluate its performance using the **ROC curve** and **AUC (Area Under the Curve)**.  \n",
    "\n",
    "The ROC curve helps visualize the **trade-off between True Positive Rate and False Positive Rate**, while the AUC gives a single number to summarize performance.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Steps in the Code\n",
    "\n",
    "### 1. Import Libraries\n",
    "- **NumPy** ‚Üí numerical operations.  \n",
    "- **Matplotlib** ‚Üí plotting.  \n",
    "- **scikit-learn** ‚Üí dataset generation, SVM model, splitting, and metrics.  \n",
    "\n",
    "### 2. Generate Dataset\n",
    "- `make_classification()` creates a **synthetic binary dataset** with:  \n",
    "  - **1000 samples**  \n",
    "  - **20 features**  \n",
    "  - **2 classes**  \n",
    "\n",
    "### 3. Split the Data\n",
    "- `train_test_split()` ‚Üí splits data into **80% training** and **20% testing**.  \n",
    "\n",
    "### 4. Train the SVM Model\n",
    "- `SVC(probability=True)` ‚Üí creates an SVM classifier with probability estimates enabled (needed for ROC).  \n",
    "- `.fit(X_train, y_train)` ‚Üí trains the model.  \n",
    "\n",
    "### 5. Predict Probabilities\n",
    "- `.predict_proba(X_test)[:, 1]` ‚Üí gets the probability of class **1** (positive class).  \n",
    "\n",
    "### 6. Compute ROC & AUC\n",
    "- `roc_curve(y_test, y_prob)` ‚Üí calculates **False Positive Rate (FPR)**, **True Positive Rate (TPR)**, and thresholds.  \n",
    "- `auc(fpr, tpr)` ‚Üí computes the **Area Under the Curve (AUC)**.  \n",
    "\n",
    "### 7. Plot ROC Curve\n",
    "- X-axis ‚Üí **False Positive Rate (FPR)**  \n",
    "- Y-axis ‚Üí **True Positive Rate (TPR)**  \n",
    "- The diagonal line (`[0,1]`) represents random guessing.  \n",
    "- The closer the ROC curve is to the **top-left corner**, the better the classifier.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "- Trained an **SVM classifier** on synthetic binary data.  \n",
    "- Evaluated performance using **ROC curve** and **AUC score**.  \n",
    "- **ROC Curve** ‚Üí shows the balance between sensitivity (TPR) and fallout (FPR).  \n",
    "- **AUC** ‚Üí a single score where **1.0 = perfect classifier**, and **0.5 = random guessing**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
